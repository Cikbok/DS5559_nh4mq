{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebooks is to generate a database for each indiviudal book, for the purpose of analysis on the four book  on the individual level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nh4mq/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nh4mq/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/nh4mq/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /home/nh4mq/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nh4mq/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import re\n",
    "import os \n",
    "import sqlite3\n",
    "from scipy.spatial.distance import pdist\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creat eText to Token Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stopwords = \"\"\"\n",
    "us rest went least would much must long one like much say well without though yet might still upon\n",
    "done every rather particular made many previous always never thy thou go first oh thee ere ye came\n",
    "almost could may sometimes seem called among another also however nevertheless even way one two three\n",
    "ever put whose therefore\n",
    "\"\"\".strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OHCO = ['chap_num', 'para_num', 'sent_num', 'token_num']\n",
    "CHAPS = OHCO[:1]\n",
    "PARAS = OHCO[:2]\n",
    "SENTS = OHCO[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_terms(vocab, no_stops=True, sort_col='n', k=1000):\n",
    "    if no_stops:\n",
    "        V = vocab[vocab.stop == 0]\n",
    "    else:\n",
    "        V = vocab\n",
    "    return V.sort_values(sort_col, ascending=False).head(k)\n",
    "\n",
    "\n",
    "\n",
    "def text_to_tokens(name,\n",
    "                   src_file,\n",
    "                   body_start=0, \n",
    "                   body_end=-1, \n",
    "                   chap_pat=r'(^\\n*(?:CHAPTER \\d+).*$)|(^\\n*(?:Chapter \\d+).*$)|(^\\n*(?:Chapter ((?=[MDCLXVI])M*D?C{0,4}L?X{0,4}V?I{0,4})).*$)|(^\\n*(?:CHAPTER ((?=[MDCLXVI])M*D?C{0,4}L?X{0,4}V?I{0,4})).*$)|(^\\n*(?:Footnote).*$)|\\\n",
    "            (^\\n*(?:INTRODUCTION).*$)|(^\\n*(?:PART ((?=[MDCLXVI])M*D?C{0,4}L?X{0,4}V?I{0,4})).*$)|(^\\s*(?:CHAPTER \\d+).*$)|(^\\s*(?:Chapter \\d+).*$)|(^\\s*(?:Chapter ((?=[MDCLXVI])M*D?C{0,4}L?X{0,4}V?I{0,4})).*$)|\\\n",
    "            (^\\n*(?:CHAPTER ((?=[MDCLXVI])M*D?C{0,4}L?X{0,4}V?I{0,4})).*$)|(^\\n*(?:Footnote).*$)|\\\n",
    "            (^\\s*(?:INTRODUCTION).*$)|(^\\s*(?:PART ((?=[MDCLXVI])M*D?C{0,4}L?X{0,4}V?I{0,4})).*$)', \n",
    "                   para_pat=r'\\n\\n+', \n",
    "                   sent_pat=r'([.;?!\"“”]+)', \n",
    "                   token_pat=r'([\\W_]+)'):\n",
    "    db_file=name+'.db'\n",
    "    # Text to lines\n",
    "    try:\n",
    "        lines = open(src_file, 'r', encoding='utf-8').readlines()\n",
    "    except:\n",
    "        lines = open(src_file, 'r', encoding='latin-1').readlines()\n",
    "    df = pd.DataFrame({'line_str':lines})\n",
    "    df.index.name = 'line_id'\n",
    "    while df[\"line_str\"][body_start]==\"\\n\":\n",
    "        body_start+=1\n",
    "    df=df[body_start:]\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    del(lines)\n",
    "    \n",
    "    # FIX CHARACTERS TO IMPROVE TOKENIZATION\n",
    "    df.line_str = df.line_str.str.replace('—', ' — ')\n",
    "    df.line_str = df.line_str.str.replace('-', ' - ')\n",
    "\n",
    "    # Lines to Chapters\n",
    "    mask = df.line_str.str.match(chap_pat)\n",
    "    df.loc[mask, 'chap_id'] = df.apply(lambda x: x.name, 1)\n",
    "    for i in range(df.shape[0]):\n",
    "        if bool(re.match('\\s*[\\r\\n]+\\s*',df.loc[i][\"line_str\"])):\n",
    "            df[\"chap_id\"][i]=np.nan\n",
    "    df[\"chap_id\"][0]=0\n",
    "    df.chap_id = df.chap_id.ffill().astype('int')\n",
    "    chap_ids = df.chap_id.unique().tolist()\n",
    "    df['chap_num'] = df.chap_id.apply(lambda x: chap_ids.index(x))\n",
    "    chaps = df.groupby('chap_num')\\\n",
    "        .apply(lambda x: ''.join(x.line_str))\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'chap_str'})\n",
    "    del(df)\n",
    "\n",
    "    # Chapters to Paragraphs\n",
    "    paras = chaps.chap_str.str.split(para_pat, expand=True)\\\n",
    "        .stack()\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'para_str'})\n",
    "    paras.index.names = PARAS\n",
    "    paras.para_str = paras.para_str.str.strip()\n",
    "    paras.para_str = paras.para_str.str.replace(r'\\n', ' ')\n",
    "    paras.para_str = paras.para_str.str.replace(r'\\s+', ' ')\n",
    "    paras = paras[~paras.para_str.str.match(r'^\\s*$')]\n",
    "    del(chaps)\n",
    "    \n",
    "    # Paragraphs to Sentences\n",
    "#     sents = paras.para_str.str.split(sent_pat, expand=True)\\\n",
    "    sents = paras.para_str\\\n",
    "        .apply(lambda x: pd.Series(nltk.sent_tokenize(x)))\\\n",
    "        .stack()\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'sent_str'})\n",
    "    sents.index.names = SENTS\n",
    "    del(paras)\n",
    "\n",
    "    # Sentences to Tokens\n",
    "#     tokens = sents.sent_str.str.split(token_pat, expand=True)\\\n",
    "    tokens = sents.sent_str\\\n",
    "        .apply(lambda x: pd.Series(nltk.pos_tag(nltk.word_tokenize(x))))\\\n",
    "        .stack()\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'pos_tuple'})\n",
    "    tokens.index.names = OHCO\n",
    "    del(sents)\n",
    "\n",
    "    tokens['pos'] = tokens.pos_tuple.apply(lambda x: x[1])\n",
    "    tokens['token_str'] = tokens.pos_tuple.apply(lambda x: x[0])\n",
    "    tokens = tokens.drop('pos_tuple', 1)\n",
    "\n",
    "    # Tag punctuation and numbers\n",
    "    tokens['punc'] = tokens.token_str.str.match(r'^[\\W_]*$').astype('int')\n",
    "    tokens['num'] = tokens.token_str.str.match(r'^.*\\d.*$').astype('int')\n",
    "    \n",
    "    # Extract vocab with minimal normalization\n",
    "    WORDS = (tokens.punc == 0) & (tokens.num == 0)\n",
    "    tokens.loc[WORDS, 'term_str'] = tokens.token_str.str.lower()\\\n",
    "        .str.replace(r'[\"_*.]', '')\n",
    "    \n",
    "    vocab = tokens[tokens.punc == 0].term_str.value_counts().to_frame()\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns={'index':'term_str', 'term_str':'n'})\n",
    "    vocab = vocab.sort_values('term_str').reset_index(drop=True)\n",
    "    vocab.index.name = 'term_id'\n",
    "    \n",
    "    # Get priors for V\n",
    "    vocab['p'] = vocab.n / vocab.n.sum()\n",
    "    \n",
    "    # Add stems\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    vocab['port_stem'] = vocab.term_str.apply(lambda x: stemmer.stem(x))\n",
    "    \n",
    "    # Define stopwords\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english') + extra_stopwords)\n",
    "    sw = pd.DataFrame({'x':1}, index=stopwords)\n",
    "    vocab['stop'] = vocab.term_str.map(sw.x).fillna(0).astype('int')\n",
    "    del(sw)\n",
    "            \n",
    "    # Add term_ids to tokens \n",
    "    tokens['term_id'] = tokens['term_str'].map(vocab.reset_index()\\\n",
    "        .set_index('term_str').term_id).fillna(-1).astype('int')\n",
    "    \n",
    "    with sqlite3.connect(db_file) as db:\n",
    "        tokens.to_sql('token', db, if_exists='replace', index=True)\n",
    "        vocab.to_sql('vocab', db, if_exists='replace', index=True)\n",
    "        \n",
    "def generate_vsm_related(dbname,alpha=.000001):\n",
    "    db_name=dbname\n",
    "    with sqlite3.connect(db_name) as db:\n",
    "        K = pd.read_sql('SELECT * FROM token', db, index_col=OHCO)\n",
    "        V = pd.read_sql('SELECT * FROM vocab', db, index_col='term_id')\n",
    "    WORDS = (K.punc == 0) & (K.num == 0) & K.term_id.isin(V[V.stop==0].index)\n",
    "    BOW = K[WORDS].groupby(OHCO[:1]+['term_id'])['term_id'].count()\n",
    "    DTM = BOW.unstack().fillna(0)\n",
    "    alpha = alpha # We introduce an arbitrary smoothing value\n",
    "    alpha_sum = alpha * V.shape[0]\n",
    "    TF = DTM.apply(lambda x: (x + alpha) / (x.sum() + alpha_sum), axis=1)\n",
    "    N_docs = DTM.shape[0]\n",
    "    V['df'] = DTM[DTM > 0].count()\n",
    "    TFIDF = TF * np.log2(N_docs / V[V.stop==0]['df'])\n",
    "    proper_nouns = K.loc[K.pos == 'NNP', 'term_id'].unique()\n",
    "    top_n = 1000\n",
    "    # TOPV = get_top_terms(vocab, sort_col='n')\n",
    "    TOPV = get_top_terms(V.loc[~V.index.isin(proper_nouns)], sort_col='n')\n",
    "    tfidf_small = TFIDF[TOPV.index].stack().to_frame().rename(columns={0:'w'})\n",
    "    THM = -(TF * np.log2(TF))\n",
    "    TFTH = TF.apply(lambda x: x * THM.sum(), 1)\n",
    "    V['tf_sum'] = TF.sum()\n",
    "    V['tf_mean'] = TF.mean()\n",
    "    V['tf_max'] = TF.max()\n",
    "    V['tfidf_sum'] = TFIDF.sum()\n",
    "    V['tfidf_mean'] = TFIDF.mean()\n",
    "    V['tfidf_max'] = TFIDF.max()\n",
    "    V['tfth_sum'] = TFTH.sum()\n",
    "    V['tfth_mean'] = TFTH.mean()\n",
    "    V['tfth_max'] = TFTH.max()\n",
    "    V['th_sum'] = THM.sum()\n",
    "    V['th_mean'] = THM.mean()\n",
    "    V['th_max'] = THM.max()\n",
    "    D = DTM.sum(1).astype('int').to_frame().rename(columns={0:'term_count'})\n",
    "    D['tf'] = D.term_count / D.term_count.sum()\n",
    "\n",
    "    with sqlite3.connect(db_name) as db:\n",
    "        V.to_sql('vocab', db, if_exists='replace', index=True)\n",
    "        K.to_sql('token', db, if_exists='replace', index=True)\n",
    "        D.to_sql('doc', db, if_exists='replace', index=True)\n",
    "        BOW.to_frame().rename(columns={'term_id':'n'}).to_sql('bow', db, if_exists='replace', index=True)\n",
    "        TFIDF.stack().to_frame().rename(columns={0:'term_weight'})\\\n",
    "        .to_sql('dtm_tfidf', db, if_exists='replace', index=True)\n",
    "        tfidf_small.to_sql('tfidf_small',db,if_exists='replace',index=True)\n",
    "        \n",
    "    \n",
    "def get_docs(tokens, div_names, doc_str = 'term_id', sep='', flatten=False, \n",
    "             index_only=False):\n",
    "    \n",
    "    if not index_only:\n",
    "        docs = tokens.groupby(div_names)[doc_str]\\\n",
    "          .apply(lambda x: x.str.cat(sep=sep))\n",
    "        docs.columns = ['doc_content']\n",
    "    else:\n",
    "        docs = tokens.groupby(div_names)[doc_str].apply(lambda x: x.tolist())\n",
    "\n",
    "    if flatten:\n",
    "        docs = docs.reset_index().drop(div_names, 1)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def get_term_id(vocab, term_str):\n",
    "    return vocab[vocab.term_str == term_str].index[0]\n",
    "\n",
    "def get_term_str(vocab, term_id):\n",
    "    return vocab.loc[term_id].term_str    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in Book Index Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_index=pd.read_csv(\"cenlab-index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop=book_index.columns[0]\n",
    "book_index.drop(drop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>filename</th>\n",
       "      <th>gender</th>\n",
       "      <th>nationality</th>\n",
       "      <th>title</th>\n",
       "      <th>words</th>\n",
       "      <th>modernist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frances Burnett</td>\n",
       "      <td>1881</td>\n",
       "      <td>1881 a fair barbarian.txt</td>\n",
       "      <td>f</td>\n",
       "      <td>a</td>\n",
       "      <td>A Fair Barbarian</td>\n",
       "      <td>41718.0</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author  date                   filename gender nationality  \\\n",
       "0  Frances Burnett  1881  1881 a fair barbarian.txt      f           a   \n",
       "\n",
       "              title    words modernist  \n",
       "0  A Fair Barbarian  41718.0         n  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_index.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the file names of the four books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles= [\"The Log of a Cowboy\",\"The Last of the Mohicans\",\"The Outlet\",\"The Mystery of the Cloomber\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=book_index[book_index[\"title\"].isin(titles)][\"filename\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the src_file and generate database for each book "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/software/standard/core/anaconda/5.2.0-py3.6/lib/python3.6/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/apps/software/standard/core/anaconda/5.2.0-py3.6/lib/python3.6/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "for i in files:\n",
    "    name=i.split('.')[0]\n",
    "    src_file=r'./texts/'+i\n",
    "    text_to_tokens(name,src_file)\n",
    "    \n",
    "###alll databases have been manually move to a file called database "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genreate BTW, TFIDF, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "OHCO = ['chap_num', 'para_num', 'sent_num', 'token_num']\n",
    "all_databases = os.listdir('./databases/')   # imagine you're one directory above test dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in all_databases:\n",
    "    src_file=r'./databases/'+i\n",
    "    generate_vsm_related(dbname=src_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chap_pairs(db_name):\n",
    "    with sqlite3.connect(db_name) as db:\n",
    "        D = pd.read_sql('SELECT * FROM doc', db, index_col=['chap_num'])\n",
    "        TFIDF = pd.read_sql('SELECT * FROM dtm_tfidf',db, index_col=[\"chap_num\",\"term_id\"])\n",
    "    TFIDF=TFIDF.unstack()\n",
    "    TFIDF.columns=TFIDF.columns.droplevel(0)\n",
    "    SIMS = pdist(TFIDF, metric='cosine')\n",
    "    pair_idx = [(i, j) for i in TFIDF.index for j in TFIDF.index if j > i]\n",
    "    doc_pairs=pd.DataFrame(columns=[\"cosine\"],index=pd.MultiIndex.from_tuples(pair_idx))\n",
    "    doc_pairs[\"cosine\"]=SIMS    \n",
    "    with sqlite3.connect(db_name) as db:\n",
    "        doc_pairs.to_sql('docpair', db, if_exists='replace', index=True)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in all_databases:\n",
    "    src_file=r'./databases/'+i\n",
    "    get_chap_pairs(db_name=src_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
